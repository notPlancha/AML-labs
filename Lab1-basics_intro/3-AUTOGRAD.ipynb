{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Azadshokrollahi/Advance-machine-learning/blob/develop/0-basics_intro/3-AUTOGRAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70Yycb3rvB9J"
   },
   "source": [
    "## AUTOMATIC DIFFERENTIATION WITH `TORCH.AUTOGRAD` ##\n",
    "\n",
    "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfANmYnovB9K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) # reference is added for backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7ORMx9uvB9K",
    "outputId": "b25bc829-7867-4549-c2e9-4db7f3ab8940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for b = None\n",
      "Gradient function for z = <AddBackward0 object at 0x000002373D6D4AF0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x000002373D6D43A0>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for b =',b.grad_fn)\n",
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2HSKDMHvB9L"
   },
   "source": [
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters. To compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad:\n",
    "\n",
    "#### We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass `retain_graph=True` to the backward call ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bVZSBldvB9L",
    "outputId": "2928515d-779c-40de-98c6-e2bdbc1679da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2419, 0.3172, 1.1146],\n",
      "        [1.2419, 0.3172, 1.1146],\n",
      "        [1.2419, 0.3172, 1.1146],\n",
      "        [1.2419, 0.3172, 1.1146],\n",
      "        [1.2419, 0.3172, 1.1146]])\n",
      "tensor([1.2419, 0.3172, 1.1146])\n"
     ]
    }
   ],
   "source": [
    "loss.backward(retain_graph=False)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC9fr1MPvB9L"
   },
   "source": [
    "By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbSIFt_lvB9L",
    "outputId": "46754443-da2f-4786-9aec-497cc8626ec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYndFnZivB9L",
    "outputId": "629ebeb1-e7f2-4a1a-bdc9-0fcdd837446f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Achieves the same result\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihH7azcuvB9L"
   },
   "source": [
    "### There are reasons you might want to disable gradient tracking: ###\n",
    "- To mark some parameters in your neural network at frozen parameters. This is a very common scenario for finetuning a pretrained network (more on this later!)\n",
    "- To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
